{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq모델 빌드를 위한 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Union\n",
    "\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./data/ChatBotData.csv_short\"\n",
    "VOCAB_PATH = \"./data/vocabulary.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 읽어오는 함수 정의\n",
    "def load_data(path: str):\n",
    "    data_df = pd.read_csv(path, header=0, sep=\",\")\n",
    "    question, answer = data_df['Q'].to_list(), data_df['A'].to_list()\n",
    "    return question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트에 대해 불필요한 문자들을 제거하고 split한다.\n",
    "def word_tokenizer(text_list: list) -> list:\n",
    "    vocab = []\n",
    "    \n",
    "    for text in text_list:\n",
    "        filter = \"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\" # filter 정의\n",
    "        CHANGE_FILTER = re.compile(filter)\n",
    "\n",
    "        cleaned_text = re.sub(CHANGE_FILTER, \"\", text) # filter에 부합하는 문자들을 제거하고\n",
    "        text_list = cleaned_text.split() # 공백을 기준으로 리스트화 하고\n",
    "        vocab.extend(text_list) # 사전에 추가한다.\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석 후 리스트에 담는다\n",
    "def preprocess_like_morphlized(data: list):\n",
    "    morph_analyzer = Okt()\n",
    "    result_data = []\n",
    "    \n",
    "    for seq in data:\n",
    "        morphed_data = \" \".join(morph_analyzer.morphs(seq))\n",
    "        result_data.append(morphed_data)\n",
    "        \n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수형 인코딩 된 형태의 사전을 리턴한다\n",
    "def make_vocabulary(vocabulary_list: list):\n",
    "    char2idx = {char : i for i, char in enumerate(vocabulary_list)}\n",
    "    idx2char = {i : char for i, char in enumerate(vocabulary_list)}\n",
    "    \n",
    "    return char2idx, idx2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs = load_data(PATH)\n",
    "\n",
    "data = inputs + outputs\n",
    "vocab2 = list(set(word_tokenizer(preprocess_like_morphlized(data)))) # 중복제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = \"<PAD>\" # padding token\n",
    "SOS = \"<SOS>\" # start of sentence token\n",
    "EOS = \"<EOS>\" # end of sentence token\n",
    "OOV = \"<OOV>\" # out of vocabulary token\n",
    "\n",
    "PAD_IDX = 0 # padding token index\n",
    "SOS_IDX = 1 # start of sentence token index\n",
    "EOS_IDX = 2 # end of sentence token index\n",
    "OOV_IDX = 3 # out of vocabulary token index\n",
    "\n",
    "# insert into vocabulary\n",
    "vocab2.insert(PAD_IDX, PAD)\n",
    "vocab2.insert(SOS_IDX, SOS)\n",
    "vocab2.insert(EOS_IDX, EOS)\n",
    "vocab2.insert(OOV_IDX, OOV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VOCAB_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in vocab2:\n",
    "        f.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = []\n",
    "\n",
    "if os.path.exists(VOCAB_PATH):\n",
    "    with open(VOCAB_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        for vocab in f.readlines():\n",
    "            vocab_list.append(vocab.replace(\"\\n\", \"\"))\n",
    "else:\n",
    "    print(f\"Cannot read {VOCAB_PATH} file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx, idx2char = make_vocabulary(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<SOS>': 1,\n",
       " '<EOS>': 2,\n",
       " '<OOV>': 3,\n",
       " '나오세요': 4,\n",
       " '다음': 5,\n",
       " '좋아요': 6,\n",
       " '하지': 7,\n",
       " '를': 8,\n",
       " '걸리겠어': 9,\n",
       " '돈': 10,\n",
       " '생각': 11,\n",
       " '불': 12,\n",
       " '선물': 13,\n",
       " '만': 14,\n",
       " '설득': 15,\n",
       " '사람': 16,\n",
       " '또': 17,\n",
       " '필요한': 18,\n",
       " '설움': 19,\n",
       " '어서': 20,\n",
       " '혼자': 21,\n",
       " '달': 22,\n",
       " '것': 23,\n",
       " '그': 24,\n",
       " '게': 25,\n",
       " '은': 26,\n",
       " '질린다': 27,\n",
       " '궁금해': 28,\n",
       " '자의': 29,\n",
       " '새': 30,\n",
       " '나': 31,\n",
       " '같아요': 32,\n",
       " '줄까': 33,\n",
       " '좋다': 34,\n",
       " '해보세요': 35,\n",
       " '가난한': 36,\n",
       " '승진': 37,\n",
       " '에': 38,\n",
       " '을': 39,\n",
       " '그럴': 40,\n",
       " '빠를수록': 41,\n",
       " '비싼데': 42,\n",
       " '있어도': 43,\n",
       " '가스': 44,\n",
       " '즐기세요': 45,\n",
       " '인데': 46,\n",
       " '생일': 47,\n",
       " '평소': 48,\n",
       " '나왔다': 49,\n",
       " '해봐요': 50,\n",
       " '거짓말': 51,\n",
       " '가상': 52,\n",
       " '비': 53,\n",
       " '바빠': 54,\n",
       " '교회': 55,\n",
       " '훈훈해': 56,\n",
       " '가': 57,\n",
       " '하세요': 58,\n",
       " '열': 59,\n",
       " '에는': 60,\n",
       " '돌아가서': 61,\n",
       " '다시': 62,\n",
       " '사세요': 63,\n",
       " '빨리': 64,\n",
       " '끄고': 65,\n",
       " '필요했던': 66,\n",
       " '좀': 67,\n",
       " '좋을까': 68,\n",
       " '잊고': 69,\n",
       " '안': 70,\n",
       " '난다': 71,\n",
       " '도': 72,\n",
       " '싶어': 73,\n",
       " '운동': 74,\n",
       " '전생': 75,\n",
       " '나라': 76,\n",
       " '나갔어': 77,\n",
       " '예요': 78,\n",
       " '뭘': 79,\n",
       " '해': 80,\n",
       " '구': 81,\n",
       " '남자친구': 82,\n",
       " '거': 83,\n",
       " '갔어': 84,\n",
       " '같아': 85,\n",
       " '적당히': 86,\n",
       " '결단': 87,\n",
       " '잘생겼어': 88,\n",
       " '출발': 89,\n",
       " '오늘': 90,\n",
       " '까지': 91,\n",
       " '땀': 92,\n",
       " '로': 93,\n",
       " '집착': 94,\n",
       " '너무': 95,\n",
       " '데려가고': 96,\n",
       " '켜놓고': 97,\n",
       " '더': 98,\n",
       " '마음': 99,\n",
       " '보인다': 100,\n",
       " '화폐': 101,\n",
       " '절약': 102,\n",
       " '감기': 103,\n",
       " '하셨나요': 104,\n",
       " '가만': 105,\n",
       " '망함': 106,\n",
       " '좋을': 107,\n",
       " '함께': 108,\n",
       " '인게': 109,\n",
       " '나온거': 110,\n",
       " '가끔': 111,\n",
       " '쫄딱': 112,\n",
       " '하는지': 113,\n",
       " '때': 114,\n",
       " '집': 115,\n",
       " '뭐': 116,\n",
       " '마세요': 117,\n",
       " '많이': 118,\n",
       " '들어올': 119,\n",
       " '식혀주세요': 120,\n",
       " '켜고': 121,\n",
       " '따라': 122,\n",
       " '따뜻하게': 123,\n",
       " '믿어줘': 124}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<PAD>',\n",
       " 1: '<SOS>',\n",
       " 2: '<EOS>',\n",
       " 3: '<OOV>',\n",
       " 4: '나오세요',\n",
       " 5: '다음',\n",
       " 6: '좋아요',\n",
       " 7: '하지',\n",
       " 8: '를',\n",
       " 9: '걸리겠어',\n",
       " 10: '돈',\n",
       " 11: '생각',\n",
       " 12: '불',\n",
       " 13: '선물',\n",
       " 14: '만',\n",
       " 15: '설득',\n",
       " 16: '사람',\n",
       " 17: '또',\n",
       " 18: '필요한',\n",
       " 19: '설움',\n",
       " 20: '어서',\n",
       " 21: '혼자',\n",
       " 22: '달',\n",
       " 23: '것',\n",
       " 24: '그',\n",
       " 25: '게',\n",
       " 26: '은',\n",
       " 27: '질린다',\n",
       " 28: '궁금해',\n",
       " 29: '자의',\n",
       " 30: '새',\n",
       " 31: '나',\n",
       " 32: '같아요',\n",
       " 33: '줄까',\n",
       " 34: '좋다',\n",
       " 35: '해보세요',\n",
       " 36: '가난한',\n",
       " 37: '승진',\n",
       " 38: '에',\n",
       " 39: '을',\n",
       " 40: '그럴',\n",
       " 41: '빠를수록',\n",
       " 42: '비싼데',\n",
       " 43: '있어도',\n",
       " 44: '가스',\n",
       " 45: '즐기세요',\n",
       " 46: '인데',\n",
       " 47: '생일',\n",
       " 48: '평소',\n",
       " 49: '나왔다',\n",
       " 50: '해봐요',\n",
       " 51: '거짓말',\n",
       " 52: '가상',\n",
       " 53: '비',\n",
       " 54: '바빠',\n",
       " 55: '교회',\n",
       " 56: '훈훈해',\n",
       " 57: '가',\n",
       " 58: '하세요',\n",
       " 59: '열',\n",
       " 60: '에는',\n",
       " 61: '돌아가서',\n",
       " 62: '다시',\n",
       " 63: '사세요',\n",
       " 64: '빨리',\n",
       " 65: '끄고',\n",
       " 66: '필요했던',\n",
       " 67: '좀',\n",
       " 68: '좋을까',\n",
       " 69: '잊고',\n",
       " 70: '안',\n",
       " 71: '난다',\n",
       " 72: '도',\n",
       " 73: '싶어',\n",
       " 74: '운동',\n",
       " 75: '전생',\n",
       " 76: '나라',\n",
       " 77: '나갔어',\n",
       " 78: '예요',\n",
       " 79: '뭘',\n",
       " 80: '해',\n",
       " 81: '구',\n",
       " 82: '남자친구',\n",
       " 83: '거',\n",
       " 84: '갔어',\n",
       " 85: '같아',\n",
       " 86: '적당히',\n",
       " 87: '결단',\n",
       " 88: '잘생겼어',\n",
       " 89: '출발',\n",
       " 90: '오늘',\n",
       " 91: '까지',\n",
       " 92: '땀',\n",
       " 93: '로',\n",
       " 94: '집착',\n",
       " 95: '너무',\n",
       " 96: '데려가고',\n",
       " 97: '켜놓고',\n",
       " 98: '더',\n",
       " 99: '마음',\n",
       " 100: '보인다',\n",
       " 101: '화폐',\n",
       " 102: '절약',\n",
       " 103: '감기',\n",
       " 104: '하셨나요',\n",
       " 105: '가만',\n",
       " 106: '망함',\n",
       " 107: '좋을',\n",
       " 108: '함께',\n",
       " 109: '인게',\n",
       " 110: '나온거',\n",
       " 111: '가끔',\n",
       " 112: '쫄딱',\n",
       " 113: '하는지',\n",
       " 114: '때',\n",
       " 115: '집',\n",
       " 116: '뭐',\n",
       " 117: '마세요',\n",
       " 118: '많이',\n",
       " 119: '들어올',\n",
       " 120: '식혀주세요',\n",
       " 121: '켜고',\n",
       " 122: '따라',\n",
       " 123: '따뜻하게',\n",
       " 124: '믿어줘'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인코더, 디코더 용 데이터 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = \"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\"\n",
    "CHANGE_FILTER = re.compile(filter)\n",
    "MAX_LEN = 25\n",
    "\n",
    "def make_sequence(value, dictionary, tokenize_as_morph = False, padding = \"pre\", position = \"\"):\n",
    "    if not tokenize_as_morph: # value 안의 각 문장들이 형태소 분석이 되어있지 않은 상태라면\n",
    "        value = preprocess_like_morphlized(value) # 위에서 정의해둔 함수로 형태소 분석을 진행한다.\n",
    "    \n",
    "    padded_sequence = [] # 패딩된 각 문장들을 담을 빈 리스트\n",
    "    for seq in value: # value안의 각 문장들을 순회하면서\n",
    "        temp_list = [] # zero padding, 인코딩된 문장 리스트를 담을 빈 리스트\n",
    "        seq_idx = [] # 문장의 각 단어가 정수형 인코딩 된 후 그 정수를 담을 빈 리스트\n",
    "        \n",
    "        seq = re.sub(CHANGE_FILTER, \"\", seq) # 불필요한 문자들을 제거하고\n",
    "        \n",
    "        # 문장 안에 있는 각 단어에 대한 정수형 인코딩 시작\n",
    "        for word in seq.split(): # 문장을 띄어쓰기 단위로 생성한 리스트의 각 단어들을 순회하면서\n",
    "            if dictionary.get(word) is not None: # 만약 그 단어가 사전에 존재하는 단어라면\n",
    "                seq_idx.append(dictionary[word]) # 사전에 할당되어 있는 숫자를 seq_idx에 append하고\n",
    "            else: # 없는 단어라면\n",
    "                seq_idx.append(dictionary[OOV]) # unknown 토큰을 append한다.\n",
    "        # 문장에 대한 정수형 인코딩 끝\n",
    "\n",
    "        # zero padding 시작\n",
    "        if len(seq_idx) > MAX_LEN: # 만약 seq_idx의 길이가 MAX_LEN보다 길다면\n",
    "            seq_idx = seq_idx[:MAX_LEN] # 최대 길이에서 자른다.\n",
    "        \n",
    "        # encoder input, encoder output, encoder target의 상황에 따라, \n",
    "        if position == \"encoder\": # encoder용 데이터라면\n",
    "            zero_padding = (MAX_LEN - len(seq_idx))*[dictionary[PAD]] # 최대 길이 - index의 길이만큼 0으로 이뤄진 패딩 리스트를 생성한다.\n",
    "        \n",
    "            if padding == \"pre\": # pre padding이라면\n",
    "                temp_list.extend(zero_padding) # zero padding값을 먼저 extend하고\n",
    "                temp_list.extend(seq_idx)\n",
    "            else: # 아니라면\n",
    "                temp_list.extend(seq_idx)\n",
    "                temp_list.extend(zero_padding) # zero padding 값을 나중에 extend한다.\n",
    "        elif position == \"decoder output\": # decoder output용 데이터라면\n",
    "            zero_padding = (MAX_LEN - len(seq_idx) - 1)*[dictionary[PAD]] # SOS토큰을 고려해서 1을 추가적으로 빼준다.\n",
    "        \n",
    "            if padding == \"pre\": # pre-padding 이라면\n",
    "                temp_list.append(dictionary[SOS]) # SOS토큰을 먼저 append 하고\n",
    "                temp_list.extend(zero_padding) # zero padding 값을 extend 한 후\n",
    "                temp_list.extend(seq_idx) # 정수형 인코딩된 문장 리스트를 extend한다.\n",
    "            else: # 아니라면\n",
    "                temp_list.append(dictionary[SOS]) # SOS토큰을 먼저 append 하고\n",
    "                temp_list.extend(seq_idx) # 정수형 인코딩된 문장 리스트를 extend한 후\n",
    "                temp_list.extend(zero_padding) # zero padding 값을 extend 한다.\n",
    "        else: # decoder target용 데이터라면\n",
    "            zero_padding = (MAX_LEN - len(seq_idx) - 1)*[dictionary[PAD]] # EOS토큰을 고려해서 1을 추가적으로 빼준다.\n",
    "        \n",
    "            if padding == \"pre\":  #pre-padding이라면\n",
    "                temp_list.extend(zero_padding) # zero padding 값을 extend 한 후\n",
    "                temp_list.extend(seq_idx) # 정수형 인코딩된 문장 리스트를 extend 하고\n",
    "                temp_list.append(dictionary[EOS]) # EOS토큰을 append한다.\n",
    "            else: # 아니라면\n",
    "                temp_list.extend(seq_idx) # 정수형 인코딩된 문장 리스트를 extend 하고\n",
    "                temp_list.extend(zero_padding) # zero padding 값을 extend 한 후\n",
    "                temp_list.append(dictionary[EOS]) # EOS토큰을 append한다.\n",
    "            \n",
    "            \n",
    "        padded_sequence.append(temp_list) # 패딩이 완료된 각 요소는 전체 요소를 담을 list에 append한다.\n",
    "    \n",
    "    return np.array(padded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = make_sequence(inputs, char2idx, position=\"encoder\")\n",
    "decoder_output = make_sequence(inputs, char2idx, position=\"decoder output\")\n",
    "decoder_target = make_sequence(inputs, char2idx, position=\"decoder target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_configs = {\n",
    "    \"char2idx\" : char2idx,\n",
    "    \"idx2char\" : idx2char,\n",
    "    'vocab_size' : len(char2idx),\n",
    "    'pad_symbol' : PAD,\n",
    "    'sos_symbol' : SOS,\n",
    "    'eos_symbol' : EOS,\n",
    "    'ovv_symbol' : OOV\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_INPUTS = \"./data/train_inputs.npy\"\n",
    "TRAIN_OUTPUTS = \"./data/train_outputs.npy\"\n",
    "TRAIN_TARGETS = \"./data/train_targets.npy\"\n",
    "DATA_CONFIGS = \"./data/data_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(TRAIN_INPUTS, encoder_input)\n",
    "np.save(TRAIN_OUTPUTS, decoder_output)\n",
    "np.save(TRAIN_TARGETS, decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(data_configs, open(DATA_CONFIGS, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Chatbot_data'...\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/songys/Chatbot_data.git"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 의류 종류 인식\n",
    "의류의 분류별로 인식하게끔 하는 과정을 진행해보자.\n",
    "\n",
    "### 필요 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import yaml\n",
    "import torch\n",
    "import shutil\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from urllib.parse import quote\n",
    "from urllib.request import urlretrieve\n",
    "from imutils.video import WebcamVideoStream\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 수집\n",
    "크롤링으로 하이버에서 종류별로 이미지를 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵션 정의\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--start-maximized')\n",
    "options.add_argument('--disable-gpu')\n",
    "options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "options.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \\\n",
    "                     AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스크롤 다운 함수 정의\n",
    "\n",
    "def scroll_down(body):\n",
    "    for _ in range(1, 4):\n",
    "        body.send_keys(Keys.PAGE_DOWN)\n",
    "        body.send_keys(Keys.PAGE_DOWN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Url 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"./114/chromedriver.exe\", options = options)\n",
    "\n",
    "label_dict = {1 : \"니트\", 2 : \"아노락\", 3 : \"후드\"}\n",
    "\n",
    "src_list = []\n",
    "for value in label_dict.values():\n",
    "    img_src = []\n",
    "    for j in range(1, 4):\n",
    "        url = f\"https://www.musinsa.com/mz/brandsnap?swh=&stx={quote(value)}&_m=&gender=&mod=&bid=&p={j}\"\n",
    "        driver.get(url)\n",
    "        driver.implicitly_wait(5)\n",
    "        body = driver.find_element(By.CSS_SELECTOR, \"body\")\n",
    "        \n",
    "        for i in range(3):\n",
    "            scroll_down(body)\n",
    "        \n",
    "        imgs = driver.find_elements(By.TAG_NAME, \"img\")\n",
    "        for img in imgs:\n",
    "            src = img.get_attribute(\"src\")\n",
    "            img_src.append(src)\n",
    "            \n",
    "    src_list.append(img_src)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수집된 URL로부터 이미지 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\" : \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \\\n",
    "           AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"}\n",
    "\n",
    "label_dict = {1 : \"knitwear\", 2 : \"anorak\", 3 : \"hoodie\"}\n",
    "\n",
    "i = 0\n",
    "for idx, srcs in enumerate(src_list):\n",
    "    for src in srcs:\n",
    "        try:\n",
    "            urlretrieve(src, f\"./images/{i}_{label_dict[idx+1]}.jpeg\")\n",
    "        except:\n",
    "            req = requests.get(src, headers = headers)\n",
    "            with open(f\"./images/{i}_{label_dict[idx+1]}.jpeg\", \"wb\") as file:\n",
    "                file.write(req.content)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_knit = []\n",
    "cnt_anorak = []\n",
    "cnt_hoodie = []\n",
    "\n",
    "for filename in os.listdir(\"./images/\"):\n",
    "    if \"knitwear\" in filename:\n",
    "        cnt_knit.append(filename)\n",
    "    elif \"anorak\" in filename:\n",
    "        cnt_anorak.append(filename)\n",
    "    elif \"hoodie\" in filename:\n",
    "        cnt_hoodie.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(\"./images/\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for to_test in cnt_knit[:15]:\n",
    "    shutil.move(f\"./images/{to_test}\", f\"./data/test/images/{to_test}\")\n",
    "    \n",
    "for to_test in cnt_anorak[:15]:\n",
    "    shutil.move(f\"./images/{to_test}\", f\"./data/test/images/{to_test}\")\n",
    "    \n",
    "for to_test in cnt_hoodie[:10]:\n",
    "    shutil.move(f\"./images/{to_test}\", f\"./data/test/images/{to_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for to_train in cnt_knit[:126]:\n",
    "    shutil.move(f\"./images/{to_train}\", f\"./data/train/images/{to_train}\")\n",
    "    \n",
    "for to_train in cnt_anorak[:126]:\n",
    "    shutil.move(f\"./images/{to_train}\", f\"./data/train/images/{to_train}\")\n",
    "    \n",
    "for to_train in cnt_hoodie[:126]:\n",
    "    shutil.move(f\"./images/{to_train}\", f\"./data/train/images/{to_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for to_valid in cnt_knit[126:]:\n",
    "    shutil.move(\n",
    "        f\"./images/{to_valid}\", \n",
    "        f\"./data/validation/images/{to_valid}\"\n",
    "    )\n",
    "    \n",
    "for to_valid in cnt_anorak[126:]:\n",
    "    shutil.move(\n",
    "        f\"./images/{to_valid}\", \n",
    "        f\"./data/validation/images/{to_valid}\"\n",
    "    )\n",
    "    \n",
    "for to_valid in cnt_hoodie[126:]:\n",
    "    shutil.move(\n",
    "        f\"./images/{to_valid}\", \n",
    "        f\"./data/validation/images/{to_valid}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 라벨링1 (1, 2중 택1)\n",
    "labelImg를 활용한 라벨링\n",
    "\n",
    "#### Download\n",
    "1. https://github.com/tzutalin/labelImg 우측 하단의 릴리즈 클릭\n",
    "2. 버전에 맞게 다운로드 후 cmd 내부에서 해당 폴더의 위치로 이동\n",
    "3. 아래 순서대로 실행\n",
    "```\n",
    ">>> pip install PyQt5\n",
    ">>> pip install lxml\n",
    ">>> pyrcc5 -o libs/resources.py resources.qrc \n",
    ">>> pip3 install labelImg \n",
    ">>> python labelimg.py\n",
    "```\n",
    "\n",
    "> pyrcc5 -o libs/resources.py resources.qrc 의 경우 pyqt에 대한 환경변수 설정이 되어 있어야 함. <p>\n",
    "> 환경변수 편집 ➡️ 시스템 변수 ➡️ 새로 만들기 ➡️ 변수이름 : pyrcc5, 변수값 : PyQt5가 설치된 경로\n",
    "\n",
    "#### Usage\n",
    "0. 설치된 폴더들을 살펴보면 data란 폴더가 있는데, 그 안에 있는 predefined_classes.txt파일을 눌러서 본인이 분류하고자 하는 클래스 이름으로 변경해야한다.\n",
    "1. `python labelimg.py`이 실행되면 GUI 창 하나가 켜진다. 좌측 하단의 Yolo 버튼을 세번 눌러서 다시 Yolo로 맞춘다.\n",
    "2. 편집할 이미지가 담긴 디렉토리를 불러온다.\n",
    "3. w를 누르면 커서가 바뀌면서 어노테이션 시작. Save버튼 누르는거 잊지 않기..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd C:\\Users\\OWNER\\Desktop\\labeling\\labelImg-master\n",
    "!python labelimg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Intel\n"
     ]
    }
   ],
   "source": [
    "%cd D:\\Intel\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 라벨링2\n",
    "roboflow를 활용한 라벨링\n",
    "\n",
    "1. Roboflow 회원가입(WorkSpace생성)\n",
    "2. 프로젝트 생성\n",
    "    + 2-1. Project Type : Object detection\n",
    "    + 2-2. What Are You Detecting? : 분류하고자 하는 것들의 대분류(여기서는 옷을 의미하는 cloth)\n",
    "    + 2-3. Project Name : 프로젝트가 생성될 이름(저는 cloth detection으로 생성했습니다.)\n",
    "    + 2-4. License : CC BY 4.0\n",
    "3. 이미지 폴더 업로드 후 우측 상단 `Save and Continue` 클릭 이후 오른쪽에서 탭 하나가 튀어나올텐데 하단에 `Assing Image`클릭\n",
    "4. 우측 상단의 `Start Annotating` 누르며 어노테이션 시작(첫 이미지 어노테이션은 바로 되는데 두번째부터는 ctrl을 누르고 진행해야함)\n",
    "5. 어노테이션 완료 후 Generate탭 클릭\n",
    "6. 차례대로 진행\n",
    "7. 우측 상단의 Export Dataset 클릭(Format 형태 : YoloV5 Pytorch, 다운로드 형태 선택)\n",
    "8. 다운된 데이터를 기반으로 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clone yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yaml 파일 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./YOLO/yolov5/data/bubble.yaml', \"r\") as f:\n",
    "    data = yaml.safe_load(f)\n",
    "    \n",
    "data[\"path\"] = r\"D:\\Intel\\BubbleData\\\\\"\n",
    "data[\"train\"] = \"train\"\n",
    "data[\"test\"] = \"test\"\n",
    "data[\"val\"] = \"validation\"\n",
    "\n",
    "data[\"nc\"] = 1\n",
    "data[\"names\"] = [\"bubble\"]\n",
    "\n",
    "with open(\"./YOLO/yolov5/data/bubble.yaml\", \"w\") as file:\n",
    "    yaml.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Intel'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련\n",
    "clone한 파일들 중 `train.py`를 이용해서 훈련을 진행한다. argparse로 작성되어 여러 옵션들을 함께 지정해주어야 한다.\n",
    "- --batch : batch_size\n",
    "- --epochs : 훈련 횟수\n",
    "- --data : data정보를 담고있는 yaml 파일\n",
    "- --weights : pretrained 된 가중치를 적용할 때 사용. 지정하지 않으면 랜덤한 가중치와 편향값들로 학습 진행\n",
    "- --cfg : 모델을 담고있는 yaml파일. s ➡️ n ➡️ m ➡️ l ➡️ x 순으로 복잡도가 다르다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=./YOLO/yolov5/yolov5s.pt, cfg=./YOLO/yolov5/models/yolov5s.yaml, data=./YOLO/yolov5/data/bubble.yaml, hyp=YOLO\\yolov5\\data\\hyps\\hyp.scratch-low.yaml, epochs=50, batch_size=2, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=0, project=YOLO\\yolov5\\runs\\train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0m YOLOv5 is out of date by 10 commits. Use 'git pull' or 'git clone https://github.com/ultralytics/yolov5' to update.\n",
      "YOLOv5  v7.0-203-g0897415 Python-3.9.12 torch-1.9.1+cu111 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir YOLO\\yolov5\\runs\\train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mWARNING  ClearML is installed but not configured, skipping ClearML logging. See https://docs.ultralytics.com/yolov5/tutorials/clearml_logging_integration#readme\n",
      "COMET WARNING: Comet credentials have not been set. Comet will default to offline logging. Please set your credentials to enable online logging.\n",
      "COMET INFO: Using 'D:\\\\Intel\\\\.cometml-runs' path as offline directory. Pass 'offline_directory' parameter into constructor or set the 'COMET_OFFLINE_DIRECTORY' environment variable to manually choose where to store offline experiment archives.\n",
      "COMET WARNING: Native output logging mode is not available, falling back to basic output logging\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "YOLOv5s summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 342/349 items from YOLO\\yolov5\\yolov5s.pt\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\Intel\\BubbleData\\train\\labels...:   0%|          | 0/35 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\Intel\\BubbleData\\train\\labels... 1 images, 0 backgrounds, 0 corrupt:   3%|▎         | 1/35 [00:05<03:23,  6.00s/it]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\Intel\\BubbleData\\train\\labels... 11 images, 0 backgrounds, 0 corrupt:  31%|███▏      | 11/35 [00:06<00:09,  2.49it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\Intel\\BubbleData\\train\\labels... 26 images, 1 backgrounds, 0 corrupt:  77%|███████▋  | 27/35 [00:06<00:01,  7.54it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\Intel\\BubbleData\\train\\labels... 34 images, 1 backgrounds, 0 corrupt: 100%|██████████| 35/35 [00:06<00:00,  5.62it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  Cache directory D:\\Intel\\BubbleData\\train is not writeable: [WinError 183]      : 'D:\\\\Intel\\\\BubbleData\\\\train\\\\labels.cache.npy' -> 'D:\\\\Intel\\\\BubbleData\\\\train\\\\labels.cache'\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Intel\\YOLO\\yolov5\\train.py\", line 647, in <module>\n",
      "    main(opt)\n",
      "  File \"d:\\Intel\\YOLO\\yolov5\\train.py\", line 536, in main\n",
      "    train(opt.hyp, opt, device, callbacks)\n",
      "  File \"d:\\Intel\\YOLO\\yolov5\\train.py\", line 213, in train\n",
      "    assert mlc < nc, f'Label class {mlc} exceeds nc={nc} in {data}. Possible class labels are 0-{nc - 1}'\n",
      "AssertionError: Label class 2 exceeds nc=1 in ./YOLO/yolov5/data/bubble.yaml. Possible class labels are 0-0\n",
      "COMET INFO: ---------------------------------------------------------------------------------------\n",
      "COMET INFO: Comet.ml OfflineExperiment Summary\n",
      "COMET INFO: ---------------------------------------------------------------------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : [OfflineExperiment will get URL after upload]\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Created from                : YOLOv5\n",
      "COMET INFO:     Name                        : exp\n",
      "COMET INFO:     comet_log_batch_metrics     : False\n",
      "COMET INFO:     comet_log_confusion_matrix  : True\n",
      "COMET INFO:     comet_log_per_class_metrics : False\n",
      "COMET INFO:     comet_max_image_uploads     : 100\n",
      "COMET INFO:     comet_mode                  : online\n",
      "COMET INFO:     comet_model_name            : yolov5\n",
      "COMET INFO:     hasNestedParams             : True\n",
      "COMET INFO:     offline_experiment          : True\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     anchor_t            : 4.0\n",
      "COMET INFO:     artifact_alias      : latest\n",
      "COMET INFO:     batch_size          : 2\n",
      "COMET INFO:     bbox_interval       : -1\n",
      "COMET INFO:     box                 : 0.05\n",
      "COMET INFO:     bucket              : \n",
      "COMET INFO:     cache               : 1\n",
      "COMET INFO:     cls                 : 0.5\n",
      "COMET INFO:     cls_pw              : 1.0\n",
      "COMET INFO:     copy_paste          : 0.0\n",
      "COMET INFO:     cos_lr              : False\n",
      "COMET INFO:     degrees             : 0.0\n",
      "COMET INFO:     device              : \n",
      "COMET INFO:     entity              : 1\n",
      "COMET INFO:     evolve              : 1\n",
      "COMET INFO:     exist_ok            : False\n",
      "COMET INFO:     fl_gamma            : 0.0\n",
      "COMET INFO:     fliplr              : 0.5\n",
      "COMET INFO:     flipud              : 0.0\n",
      "COMET INFO:     freeze              : [0]\n",
      "COMET INFO:     hsv_h               : 0.015\n",
      "COMET INFO:     hsv_s               : 0.7\n",
      "COMET INFO:     hsv_v               : 0.4\n",
      "COMET INFO:     hyp|anchor_t        : 4.0\n",
      "COMET INFO:     hyp|box             : 0.05\n",
      "COMET INFO:     hyp|cls             : 0.5\n",
      "COMET INFO:     hyp|cls_pw          : 1.0\n",
      "COMET INFO:     hyp|copy_paste      : 0.0\n",
      "COMET INFO:     hyp|degrees         : 0.0\n",
      "COMET INFO:     hyp|fl_gamma        : 0.0\n",
      "COMET INFO:     hyp|fliplr          : 0.5\n",
      "COMET INFO:     hyp|flipud          : 0.0\n",
      "COMET INFO:     hyp|hsv_h           : 0.015\n",
      "COMET INFO:     hyp|hsv_s           : 0.7\n",
      "COMET INFO:     hyp|hsv_v           : 0.4\n",
      "COMET INFO:     hyp|iou_t           : 0.2\n",
      "COMET INFO:     hyp|lr0             : 0.01\n",
      "COMET INFO:     hyp|lrf             : 0.01\n",
      "COMET INFO:     hyp|mixup           : 0.0\n",
      "COMET INFO:     hyp|momentum        : 0.937\n",
      "COMET INFO:     hyp|mosaic          : 1.0\n",
      "COMET INFO:     hyp|obj             : 1.0\n",
      "COMET INFO:     hyp|obj_pw          : 1.0\n",
      "COMET INFO:     hyp|perspective     : 0.0\n",
      "COMET INFO:     hyp|scale           : 0.5\n",
      "COMET INFO:     hyp|shear           : 0.0\n",
      "COMET INFO:     hyp|translate       : 0.1\n",
      "COMET INFO:     hyp|warmup_bias_lr  : 0.1\n",
      "COMET INFO:     hyp|warmup_epochs   : 3.0\n",
      "COMET INFO:     hyp|warmup_momentum : 0.8\n",
      "COMET INFO:     hyp|weight_decay    : 0.0005\n",
      "COMET INFO:     image_weights       : False\n",
      "COMET INFO:     imgsz               : 640\n",
      "COMET INFO:     iou_t               : 0.2\n",
      "COMET INFO:     label_smoothing     : 0.0\n",
      "COMET INFO:     local_rank          : -1\n",
      "COMET INFO:     lr0                 : 0.01\n",
      "COMET INFO:     lrf                 : 0.01\n",
      "COMET INFO:     mixup               : 0.0\n",
      "COMET INFO:     momentum            : 0.937\n",
      "COMET INFO:     mosaic              : 1.0\n",
      "COMET INFO:     multi_scale         : False\n",
      "COMET INFO:     name                : exp\n",
      "COMET INFO:     noautoanchor        : False\n",
      "COMET INFO:     noplots             : False\n",
      "COMET INFO:     nosave              : False\n",
      "COMET INFO:     noval               : False\n",
      "COMET INFO:     obj                 : 1.0\n",
      "COMET INFO:     obj_pw              : 1.0\n",
      "COMET INFO:     optimizer           : SGD\n",
      "COMET INFO:     patience            : 100\n",
      "COMET INFO:     perspective         : 0.0\n",
      "COMET INFO:     project             : YOLO\\yolov5\\runs\\train\n",
      "COMET INFO:     quad                : False\n",
      "COMET INFO:     rect                : False\n",
      "COMET INFO:     resume              : False\n",
      "COMET INFO:     save_dir            : YOLO\\yolov5\\runs\\train\\exp20\n",
      "COMET INFO:     save_period         : -1\n",
      "COMET INFO:     scale               : 0.5\n",
      "COMET INFO:     seed                : 0\n",
      "COMET INFO:     shear               : 0.0\n",
      "COMET INFO:     single_cls          : False\n",
      "COMET INFO:     sync_bn             : False\n",
      "COMET INFO:     translate           : 0.1\n",
      "COMET INFO:     upload_dataset      : False\n",
      "COMET INFO:     val_conf_threshold  : 0.001\n",
      "COMET INFO:     val_iou_threshold   : 0.6\n",
      "COMET INFO:     warmup_bias_lr      : 0.1\n",
      "COMET INFO:     warmup_epochs       : 3.0\n",
      "COMET INFO:     warmup_momentum     : 0.8\n",
      "COMET INFO:     weight_decay        : 0.0005\n",
      "COMET INFO:     workers             : 0\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     asset                        : 3 (1.23 KB)\n",
      "COMET INFO:     conda-environment-definition : 1\n",
      "COMET INFO:     conda-info                   : 1\n",
      "COMET INFO:     conda-specification          : 1\n",
      "COMET INFO:     environment details          : 1\n",
      "COMET INFO:     git metadata                 : 1\n",
      "COMET INFO:     installed packages           : 1\n",
      "COMET INFO: \n",
      "COMET INFO: Still saving offline stats to messages file before program termination (may take up to 120 seconds)\n",
      "COMET INFO: Starting saving the offline archive\n",
      "COMET INFO: To upload this offline experiment, run:\n",
      "    comet upload D:\\Intel\\.cometml-runs\\0f09b116912346c395106f0858b44e6e.zip\n"
     ]
    }
   ],
   "source": [
    "!python ./YOLO/yolov5/train.py --batch 2 --epochs 100 --data ./YOLO/yolov5/data/bubble.yaml  --weights ./YOLO/yolov5/yolov5m.pt --cfg ./YOLO/yolov5/models/yolov5m.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검출\n",
    "위에서 훈련한 나만의 모델을 기반으로 새로운 이미지들에 대한 객체 탐지 결과를 `detect.py`파일로 받아본다. 마찬가지로 argparse로 작성되어있다.\n",
    "- --source : 탐지할 이미지 경로\n",
    "- --weights : 나의 학습 모델의 가중치 경로\n",
    "- --conf : 객체 탐지 신뢰도. 해당 신뢰도 이상의 객체들만 탐지한다. 기본값은 0.4이며 높을 수록 탐지되는 객체의 수도 적어진다.\n",
    "- --name : 탐지된 파일들을 저장할 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['./YOLO/yolov5/runs/train/exp24/weights/best.pt'], source=D:\\Intel\\BubbleData\\test\\images, data=YOLO\\yolov5\\data\\coco128.yaml, imgsz=[640, 640], conf_thres=0.65, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=YOLO\\yolov5\\runs\\detect, name=C:\\Users\\OWNER\\Desktop\\bubble_detection_test\\results, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5  v7.0-203-g0897415 Python-3.9.12 torch-1.9.1+cu111 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 212 layers, 20852934 parameters, 0 gradients, 47.9 GFLOPs\n",
      "image 1/17 D:\\Intel\\BubbleData\\test\\images\\10c.jpg: 352x640 34 bubbles, 14.6ms\n",
      "image 2/17 D:\\Intel\\BubbleData\\test\\images\\11c.jpg: 480x640 (no detections), 13.8ms\n",
      "image 3/17 D:\\Intel\\BubbleData\\test\\images\\50c.jpg: 384x640 29 bubbles, 16.0ms\n",
      "image 4/17 D:\\Intel\\BubbleData\\test\\images\\51c.jpg: 384x640 23 bubbles, 11.7ms\n",
      "image 5/17 D:\\Intel\\BubbleData\\test\\images\\52c.jpg: 320x640 16 bubbles, 14.5ms\n",
      "image 6/17 D:\\Intel\\BubbleData\\test\\images\\53c.jpg: 640x608 12 bubbles, 16.1ms\n",
      "image 7/17 D:\\Intel\\BubbleData\\test\\images\\54c.jpg: 448x640 29 bubbles, 14.2ms\n",
      "image 8/17 D:\\Intel\\BubbleData\\test\\images\\55c.jpg: 416x640 1 bubble, 14.0ms\n",
      "image 9/17 D:\\Intel\\BubbleData\\test\\images\\KakaoTalk_20230904_135459407_24.jpg: 480x640 15 bubbles, 14.0ms\n",
      "image 10/17 D:\\Intel\\BubbleData\\test\\images\\KakaoTalk_20230904_135459407_25.jpg: 480x640 12 bubbles, 13.8ms\n",
      "image 11/17 D:\\Intel\\BubbleData\\test\\images\\KakaoTalk_20230904_140234299_10.jpg: 640x480 (no detections), 13.9ms\n",
      "image 12/17 D:\\Intel\\BubbleData\\test\\images\\KakaoTalk_20230904_140234299_11.jpg: 640x480 2 bubbles, 13.3ms\n",
      "image 13/17 D:\\Intel\\BubbleData\\test\\images\\KakaoTalk_20230904_140234299_12.jpg: 640x480 (no detections), 13.4ms\n",
      "image 14/17 D:\\Intel\\BubbleData\\test\\images\\j2.jpg: 640x480 6 bubbles, 13.7ms\n",
      "image 15/17 D:\\Intel\\BubbleData\\test\\images\\k.jpg: 448x640 53 bubbles, 13.6ms\n",
      "image 16/17 D:\\Intel\\BubbleData\\test\\images\\k2.jpg: 640x480 (no detections), 13.7ms\n",
      "image 17/17 D:\\Intel\\BubbleData\\test\\images\\l.jpg: 448x640 50 bubbles, 14.0ms\n",
      "Speed: 0.4ms pre-process, 14.0ms inference, 1.3ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mC:\\Users\\OWNER\\Desktop\\bubble_detection_test\\results3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python ./YOLO/yolov5/detect.py --source \"D:\\Intel\\BubbleData\\test\\images\" --weights \"./YOLO/yolov5/runs/train/exp26/weights/best.pt\" --conf 0.65 --name \"C:\\Users\\OWNER\\Desktop\\bubble_detection_test\\results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-203-g0897415 Python-3.9.12 torch-1.9.1+cu111 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 212 layers, 20852934 parameters, 0 gradients, 47.9 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     frame \u001b[39m=\u001b[39m cam\u001b[39m.\u001b[39mread()\n\u001b[1;32m---> 10\u001b[0m     results \u001b[39m=\u001b[39m model(frame)\n\u001b[0;32m     11\u001b[0m     results\u001b[39m.\u001b[39mrender()\n\u001b[0;32m     12\u001b[0m     cv2\u001b[39m.\u001b[39mimshow(\u001b[39m'\u001b[39m\u001b[39mModi Camera Module\u001b[39m\u001b[39m'\u001b[39m, frame)\n",
      "File \u001b[1;32mc:\\Users\\OWNER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\OWNER\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     27\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[1;32m---> 28\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Intel\\./YOLO/yolov5\\models\\common.py:689\u001b[0m, in \u001b[0;36mAutoShape.forward\u001b[1;34m(self, ims, size, augment, profile)\u001b[0m\n\u001b[0;32m    687\u001b[0m     im, f \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(exif_transpose(im)), \u001b[39mgetattr\u001b[39m(im, \u001b[39m'\u001b[39m\u001b[39mfilename\u001b[39m\u001b[39m'\u001b[39m, f) \u001b[39mor\u001b[39;00m f\n\u001b[0;32m    688\u001b[0m files\u001b[39m.\u001b[39mappend(Path(f)\u001b[39m.\u001b[39mwith_suffix(\u001b[39m'\u001b[39m\u001b[39m.jpg\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mname)\n\u001b[1;32m--> 689\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39m] \u001b[39m<\u001b[39m \u001b[39m5\u001b[39m:  \u001b[39m# image in CHW\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mtranspose((\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m))  \u001b[39m# reverse dataloader .transpose(2, 0, 1)\u001b[39;00m\n\u001b[0;32m    691\u001b[0m im \u001b[39m=\u001b[39m im[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, :\u001b[39m3\u001b[39m] \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39melse\u001b[39;00m cv2\u001b[39m.\u001b[39mcvtColor(im, cv2\u001b[39m.\u001b[39mCOLOR_GRAY2BGR)  \u001b[39m# enforce 3ch input\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('./YOLO/yolov5', 'custom', path = \"./YOLO/yolov5/runs/train/exp24/weights/best.pt\", source = \"local\")\n",
    "model.conf = 0.65\n",
    "\n",
    "esp_ip = \"http://192.168.0.39\"\n",
    "host = \"{}:4747/video\".format(esp_ip)      \n",
    "cam = WebcamVideoStream(src=host).start()    \n",
    "\n",
    "while True:\n",
    "    frame = cam.read()\n",
    "    results = model(frame)\n",
    "    results.render()\n",
    "    cv2.imshow('Modi Camera Module', frame)\n",
    "    if cv2.waitKey(1) & 0xff == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과\n",
    "학습된 모델의 상태가 좋지 않다. 원인을 따져보자면 아래와 같을 것이다.\n",
    "- 클래스 간의 상관관계가 너무 높음\n",
    "    + 현재 구성한 클래스는 [반팔, 긴팔, 맨투맨, 후드, 니트]로, 니트의 경우 반팔 니트, 긴팔 니트가 존재하는 등의 상관관계가 높은 것이 아닌가 하는 생각이 든다\n",
    "- 노이즈\n",
    "    + Roboflow에서 제공하는 이미지 증강이 오히려 역효과를 낸 건 아닐까 하는 마음.. \n",
    "    + https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=4u_olion&logNo=221437862590 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 보완\n",
    "기존에 roboflow로 노이즈도 섞고 해서 나온 약 900장 가량의 데이터로 학습 시킨 데 반해, 이번에는 labelImg를 통해 어노테이션만 진행하되 기존의 이미지 형식과 다른 마켓 진열 사진이 아닌 사람이 입고 있는 사진에 별도의 클래스들을 추가했다.\n",
    "\n",
    "각 데이터셋의 개수는 아래와 같다.\n",
    "- train : 315장\n",
    "- test : 81장\n",
    "- validation : 122장\n",
    "\n",
    "아래는 바뀐 클래스 명이다.\n",
    "\n",
    "|변경 전|변경 후|\n",
    "|---|---|\n",
    "|['long', 'short', 'man2man', 'knitwear', 'hoodie']|[\"knitwear\", \"anorak\", \"hoodie\", \"jeans\", \"long pants\", \"short pants\", \"skirt\"]|\n",
    "\n",
    "단순히 상의에 국한되지 않고 하의 정보까지 추가하여 어노테이션을 진행하고, 별도의 이미지 증강 처리 없이 학습을 진행했다.\n",
    "\n",
    "총 두 번의 시도가 있었는데, 변경점들만 짧게 살펴본다.\n",
    "\n",
    "1회차\n",
    "\n",
    "|변경점|내용|\n",
    "|---|---|\n",
    "|epochs|횟수 감소(200 ➡️ 100)|\n",
    "|valid, test|모두 train set으로 진행|\n",
    "\n",
    "2회차\n",
    "\n",
    "|변경점|내용|\n",
    "|---|---|\n",
    "|epochs|횟수 증가(100 ➡️ 150)|\n",
    "|weights|yolov5n ➡️ yolov5m으로 변경|\n",
    "|valid, test|별도의 valid, test셋으로 진행|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

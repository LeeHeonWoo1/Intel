{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yolo Mini Project\n",
    "비눗방울 객체탐지\n",
    "\n",
    "1차적으로 데이터를 수집하여 annotation을 1차적으로 진행했으며, 수집한 방식은 아래와 같다.\n",
    "- 구글에서 약 50장 가량의 데이터 수집하고, 다이소에서 구매한 버블건을 활용하여 실제 사진을 찍기도 함\n",
    "- LabelImg를 활용한 어노테이션 진행\n",
    "- 72장의 train images, 21개의 validation images, 17개의 test images 수집 완료\n",
    "\n",
    "사전에 훈련을 진행해보았을 때, 너무 작은 비눗방울의 경우 제대로된 탐지를 하지 못하는 문제점이 있었다.\n",
    "\n",
    "즉시 어노테이션 파일을 점검한 결과로 어노테이션을 수정하고, 좀 더 타이트하게 어노테이션을 진행했다. \n",
    "\n",
    "<div align=center><img src=\"./Wrong Annotation.png\"></div>\n",
    "\n",
    "이후 학습을 진행했을 때 작은 물체까지 잘 잡아내는 모습을 보였으나, 이에 대한 정확도를 높이기 위해 추가적으로 데이터를 수집한다.\n",
    "\n",
    "### 필요 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import yaml\n",
    "import torch\n",
    "import shutil\n",
    "from crawler.pix_crawling import PixCrawler  # 직접 만든 클래스\n",
    "from imutils.video import WebcamVideoStream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Url 수집\n",
    "기존의 이미지와 픽사베이에서 수집한 이미지를 병합한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Bubble 데이터 다운로드를 시작합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "percentage: 100%|===================| 176/176 [00:16<00:00, 10.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bubble 176개 중 176개 이미지를 다운받았습니다. \n",
      "================================================================================\n",
      "이미지 다운로드가 완료되었습니다. ./Append폴더를 확인해주세요. \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "users_input = [\"Bubble\"]  # 여러개를 검색하려면 요소 추가할 것\n",
    "keywords = users_input.split(\", \")\n",
    "\n",
    "for key in keywords:\n",
    "    if key.startswith(\" \"):\n",
    "        key.replace(key[0], \"\")\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "my_crawler = PixCrawler(\n",
    "    keywords=keywords,\n",
    "    num_of_data=200,\n",
    "    path_datasets=\"./Append\",\n",
    "    path_output_datasets=\"./output\",\n",
    ")\n",
    "my_crawler.dowload_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 1차 전처리\n",
    "이미 있는 데이터들이거나, 상관없는 이미지들의 경우 삭제한다. \n",
    "\n",
    "### 이미지 이동\n",
    "176장 중 불필요한 데이터를 제거하고 총 34장의 데이터를 확보했다. 이전 결과와 똑같이 비교하기 위해 28장은 훈련셋 이미지로, 6장은 검증셋 이미지로 사용하며 테스트셋에는 별도로 더 추가하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"./Append/Bubble\"\n",
    "image_list = os.listdir(image_dir)\n",
    "\n",
    "for img in image_list[:28]:\n",
    "    shutil.move(f\"{image_dir}/{img}\", f\"./BubbleData/train/images/{img}\")\n",
    "\n",
    "for img2 in image_list[28:]:\n",
    "    shutil.move(f\"{image_dir}/{img2}\", f\"./BubbleData/validation/images/{img2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추가 데이터에 대한 라벨링 작업\n",
    "추가적인 데이터에 대한 라벨링 작업을 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd C:\\Users\\OWNER\\Desktop\\labeling\\labelImg-master\n",
    "!python labelimg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Intel\\YoloProject\n"
     ]
    }
   ],
   "source": [
    "%cd D:\\Intel\\YoloProject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clone yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yaml 파일 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./yolov5/data/coco.yaml\", \"r\") as f:\n",
    "    data = yaml.safe_load(f)\n",
    "\n",
    "data[\"path\"] = r\"D:\\Intel\\YoloProject\\BubbleData\"\n",
    "data[\"train\"] = \"train\"\n",
    "data[\"test\"] = \"test\"\n",
    "data[\"val\"] = \"validation\"\n",
    "\n",
    "data[\"nc\"] = 1\n",
    "data[\"names\"] = [\"bubble\"]\n",
    "\n",
    "with open(\"./yolov5/data/coco.yaml\", \"w\") as file:\n",
    "    yaml.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Intel\\\\YoloProject'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련\n",
    "clone한 파일들 중 `train.py`를 이용해서 훈련을 진행한다. argparse로 작성되어 여러 옵션들을 함께 지정해주어야 한다.\n",
    "- --batch : batch_size\n",
    "- --epochs : 훈련 횟수\n",
    "- --data : data정보를 담고있는 yaml 파일\n",
    "- --weights : pretrained 된 가중치를 적용할 때 사용. 지정하지 않으면 랜덤한 가중치와 편향값들로 학습 진행. 해당 파일이 존재하지 않을 경우 다운로드를 받고 사용한다.\n",
    "- --cfg : 모델을 담고있는 yaml파일. s ➡️ n ➡️ m ➡️ l ➡️ x 순으로 복잡도가 다르다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./yolov5/train.py --batch 2 --epochs 150 --data ./yolov5/data/coco.yaml  --weights ./yolov5/yolov5m.pt --cfg ./yolov5/models/yolov5m.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검출\n",
    "위에서 훈련한 나만의 모델을 기반으로 새로운 이미지들에 대한 객체 탐지 결과를 `detect.py`파일로 받아본다. 마찬가지로 argparse로 작성되어있다.\n",
    "- --source : 탐지할 이미지 경로\n",
    "- --weights : 나의 학습 모델의 가중치 경로\n",
    "- --conf : 객체 탐지 신뢰도. 해당 신뢰도 이상의 객체들만 탐지한다. 기본값은 0.4이며 높을 수록 탐지되는 객체의 수도 적어진다.\n",
    "- --name : 탐지된 파일들을 저장할 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./yolov5/detect.py --source \"D:\\Intel\\YoloProject\\BubbleData\\test\\images\" --weights \"./yolov5/runs/train/exp/weights/best.pt\" --conf 0.65 --name \"C:\\Users\\OWNER\\Desktop\\bubble_detection_test\\results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Droid Cam을 활용한 객체 탐지\n",
    "Droid Cam에 Custom data의 pt파일을 적용시켜 실시간으로 검출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-214-g8c30c58 Python-3.9.12 torch-1.9.1+cu111 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 212 layers, 20852934 parameters, 0 gradients, 47.9 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     frame \u001b[39m=\u001b[39m cam\u001b[39m.\u001b[39mread()\n\u001b[1;32m---> 10\u001b[0m     results \u001b[39m=\u001b[39m model(frame)\n\u001b[0;32m     11\u001b[0m     results\u001b[39m.\u001b[39mrender()\n\u001b[0;32m     12\u001b[0m     cv2\u001b[39m.\u001b[39mimshow(\u001b[39m'\u001b[39m\u001b[39mModi Camera Module\u001b[39m\u001b[39m'\u001b[39m, frame)\n",
      "File \u001b[1;32mc:\\Users\\OWNER\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\OWNER\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     27\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[1;32m---> 28\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Intel\\YoloProject\\./yolov5\\models\\common.py:710\u001b[0m, in \u001b[0;36mAutoShape.forward\u001b[1;34m(self, ims, size, augment, profile)\u001b[0m\n\u001b[0;32m    708\u001b[0m     ims[i] \u001b[39m=\u001b[39m im \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mcontiguous \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mascontiguousarray(im)  \u001b[39m# update\u001b[39;00m\n\u001b[0;32m    709\u001b[0m shape1 \u001b[39m=\u001b[39m [make_divisible(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39marray(shape1)\u001b[39m.\u001b[39mmax(\u001b[39m0\u001b[39m)]  \u001b[39m# inf shape\u001b[39;00m\n\u001b[1;32m--> 710\u001b[0m x \u001b[39m=\u001b[39m [letterbox(im, shape1, auto\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m im \u001b[39min\u001b[39;00m ims]  \u001b[39m# pad\u001b[39;00m\n\u001b[0;32m    711\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mascontiguousarray(np\u001b[39m.\u001b[39marray(x)\u001b[39m.\u001b[39mtranspose((\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)))  \u001b[39m# stack and BHWC to BCHW\u001b[39;00m\n\u001b[0;32m    712\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(x)\u001b[39m.\u001b[39mto(p\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39mtype_as(p) \u001b[39m/\u001b[39m \u001b[39m255\u001b[39m  \u001b[39m# uint8 to fp16/32\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Intel\\YoloProject\\./yolov5\\models\\common.py:710\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    708\u001b[0m     ims[i] \u001b[39m=\u001b[39m im \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mcontiguous \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mascontiguousarray(im)  \u001b[39m# update\u001b[39;00m\n\u001b[0;32m    709\u001b[0m shape1 \u001b[39m=\u001b[39m [make_divisible(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39marray(shape1)\u001b[39m.\u001b[39mmax(\u001b[39m0\u001b[39m)]  \u001b[39m# inf shape\u001b[39;00m\n\u001b[1;32m--> 710\u001b[0m x \u001b[39m=\u001b[39m [letterbox(im, shape1, auto\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m im \u001b[39min\u001b[39;00m ims]  \u001b[39m# pad\u001b[39;00m\n\u001b[0;32m    711\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mascontiguousarray(np\u001b[39m.\u001b[39marray(x)\u001b[39m.\u001b[39mtranspose((\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)))  \u001b[39m# stack and BHWC to BCHW\u001b[39;00m\n\u001b[0;32m    712\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(x)\u001b[39m.\u001b[39mto(p\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39mtype_as(p) \u001b[39m/\u001b[39m \u001b[39m255\u001b[39m  \u001b[39m# uint8 to fp16/32\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Intel\\YoloProject\\./yolov5\\utils\\augmentations.py:140\u001b[0m, in \u001b[0;36mletterbox\u001b[1;34m(im, new_shape, color, auto, scaleFill, scaleup, stride)\u001b[0m\n\u001b[0;32m    138\u001b[0m top, bottom \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mround\u001b[39m(dh \u001b[39m-\u001b[39m \u001b[39m0.1\u001b[39m)), \u001b[39mint\u001b[39m(\u001b[39mround\u001b[39m(dh \u001b[39m+\u001b[39m \u001b[39m0.1\u001b[39m))\n\u001b[0;32m    139\u001b[0m left, right \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mround\u001b[39m(dw \u001b[39m-\u001b[39m \u001b[39m0.1\u001b[39m)), \u001b[39mint\u001b[39m(\u001b[39mround\u001b[39m(dw \u001b[39m+\u001b[39m \u001b[39m0.1\u001b[39m))\n\u001b[1;32m--> 140\u001b[0m im \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mcopyMakeBorder(im, top, bottom, left, right, cv2\u001b[39m.\u001b[39;49mBORDER_CONSTANT, value\u001b[39m=\u001b[39;49mcolor)  \u001b[39m# add border\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mreturn\u001b[39;00m im, ratio, (dw, dh)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = torch.hub.load(\n",
    "    \"./yolov5\", \"custom\", path=\"./yolov5/runs/train/exp/weights/best.pt\", source=\"local\"\n",
    ")\n",
    "model.conf = 0.65\n",
    "\n",
    "esp_ip = \"http://192.168.137.213\"\n",
    "host = \"{}:4747/video\".format(esp_ip)\n",
    "cam = WebcamVideoStream(src=host).start()\n",
    "\n",
    "while True:\n",
    "    frame = cam.read()\n",
    "    results = model(frame)\n",
    "    results.render()\n",
    "    cv2.imshow(\"Modi Camera Module\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
